#!/bin/bash

source $STI_SCRIPTS_PATH/s2i-env-vars

# Just a word about the directory structure
# SPARK_HOME == /opt/spark
# SPARK_INSTALL == /opt/spark-distro

# Extra things like default configuration files and additional
# boot scripts may be stored in SPARK_INSTALL

# At runtime, /opt/spark is a symlink to /opt/spark-distro/distro
# but /opt/spark-distro/distro does not actually exist yet

# The Spark tarball will be expanded in /opt/spark-distro using
# it's original name, for example /opt/spark-distro/spark-2.3.0-bin-hadoop2.7,
# as a dev aid to tracking and version checking

# Ultimately, /opt/spark-distro/distro is created as a symlink to the Spark root
# directory. This double-hop from /opt/spark to the Spark root through symlinks
# allows the Spark installation to be staged in the base image but completed in
# the S2I build without expanding permissions

function match_sum {
    local sumfile=$1
    local delim=$2
    local md5=$3
    local initial=$(cat $sumfile | cut -d"$delim" -f1 | tr [:upper:] [:lower:] | tr -d [:space:])
    local rest=$(cat $sumfile | cut -d"$delim" --complement -f1 | tr [:upper:] [:lower:] | tr -d [:space:])
    if [ "$md5" == "$initial" ] || [ "$md5" == "$rest" ]; then
        return 0
    fi
    return 1
}

if [ -f $SPARK_HOME/bin/spark-submit ]; then
    echo "Spark is installed, nothing to do"
    exit 1
else
    echo "Attempting to install Spark"
    # If a url has been specfified for spark use it
    if [ -n "$SPARK_URL" ]; then
        echo Downloading $SPARK_URL
        wget $SPARK_URL -P $S2I_SOURCE_DIR
    fi
    if [ -n "$SPARK_MD5_URL" ]; then
        echo Downloading $SPARK_MD5_URL
        wget $SPARK_MD5_URL -P $S2I_SOURCE_DIR
    fi

    for spark in $(ls "$S2I_SOURCE_DIR"); do

        spark=$S2I_SOURCE_DIR/$spark

        # Is the file a directory? If it contains spark-submit, move it
        if [ -d "$spark" ]; then
            if ! [ -f $spark/bin/spark-submit ]; then
                echo Ignoring directory $spark, no spark-submit
                continue
            fi
            echo Installing from directory $spark
            sparkdir=$SPARK_INSTALL/$(basename $spark)
            mv $spark $SPARK_INSTALL
        else
            # If we can get the table of contents, it's a tar archive, otherwise ignore
            tar -tf $spark &> /dev/null
            if [ "$?" -ne 0 ]; then
                echo Ignoring $spark, not a tar archive
                continue
            fi

            # Does the tarball contain a spark-submit?
            name=$(tar -tzf $spark | grep -e "^[^/]*/bin/spark-submit$")
            if [ "$?" -ne 0 ]; then
                echo Ignoring tarball $spark, no spark-submit
                continue
            else
                # See if we have an md5 file to match against
                if [ -f "$spark".md5 ]; then
                    calcvalue=$(md5sum "$spark" | cut -d\  -f1)
                    # split the md5 file using a space
                    match_sum "$spark".md5 \  $calcvalue
                    matched="$?"
                    if [ "$matched" -ne 0 ]; then
                        # split the md5 file using equals sign in case it's BSD
                        match_sum "$spark".md5 \=  $calcvalue
                        matched="$?"
                    fi
                    if [ "$matched" -ne 0 ]; then
                        echo Ignoring tarball $spark, md5sum did not match
                        continue
                    fi
                fi

                # dname will be the intial directory from the path of spark-submit
                # we found in the tarball, ie the dir created by tar
                echo Installing from tarball $spark
                dname=$(dirname $name | cut -d/ -f 1)
                sparkdir=$SPARK_INSTALL/$dname
                tar -xzf $spark -C $SPARK_INSTALL
            fi
        fi

        ln -s $sparkdir $SPARK_INSTALL/distro
        # Spark workers need to write to the spark directory to track apps
        chmod -R g+rwX $sparkdir

        # Search for the spark entrypoint file and copy it to $SPARK_INSTALL
        entry=$(find $sparkdir/kubernetes -name entrypoint.sh)
        if [ -n "$entry" ]; then
            echo Installing spark native entrypoint for use with spark-on-k8s commands
            cp $entry $SPARK_INSTALL

            # We have to patch the entrypoint to toggle error checking
            # around the uidentry check for 2.3 (fix on the way)
            sed -i '/^uidentry/i set +e' $SPARK_INSTALL/entrypoint.sh
            sed -i '/^uidentry/a set -e' $SPARK_INSTALL/entrypoint.sh

            # We also want to get rid of the tini invocation
            sed -i "s@exec /sbin/tini -s --@exec@" $SPARK_INSTALL/entrypoint.sh
        else
            echo No spark native entrypoint found for use with spark-on-k8s commands
        fi

	# Include the default spark configuration files
        pushd $SPARK_INSTALL/conf &> /dev/null
        for conf_file in *; do
            if ! [ -f "$SPARK_HOME"/conf/$conf_file ]; then
	        echo Moving $conf_file to $SPARK_HOME/conf
                mv $conf_file $SPARK_HOME/conf
            else
	        echo Not moving $conf_file, $SPARK_HOME/conf/$conf_file already exists
            fi
        done
        popd &> /dev/null

        # Can we run spark-submit?
        $SPARK_HOME/bin/spark-submit --version
        if [ "$?" -eq 0 ]; then
            echo Spark installed successfully
            exit 0
	else
            echo Cannot run spark-submit, Spark install failed
        fi

        # Just in case there is more than one tarball, clean up
        rm -rf $sparkdir
    done

    echo no valid Spark distribution found

    if [ -n "$DEBUG_ASSEMBLE" ]; then
        echo Looping forever so you can \'oc rsh\'
        while true; do
            sleep 5
        done
    fi
    exit 1
fi
