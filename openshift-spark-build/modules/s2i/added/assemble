#!/bin/bash

source $STI_SCRIPTS_PATH/s2i-env-vars

# Just a word about the directory structure
# SPARK_HOME == /opt/spark
# SPARK_INSTALL == /opt/spark-distro

# Extra things like default configuration files and additional
# boot scripts may be stored in SPARK_INSTALL

# At runtime, /opt/spark is a symlink to /opt/spark-distro/distro
# but /opt/spark-distro/distro does not actually exist yet

# The Spark tarball will be expanded in /opt/spark-distro using
# it's original name, for example /opt/spark-distro/spark-2.3.0-bin-hadoop2.7,
# as a dev aid to tracking and version checking

# Ultimately, /opt/spark-distro/distro is created as a symlink to the Spark root
# directory. This double-hop from /opt/spark to the Spark root through symlinks
# allows the Spark installation to be staged in the base image but completed in
# the S2I build without expanding permissions

function match_sum {
    local sumfile=$1
    local delim=$2
    local sha512=$3
    local initial=$(cat $sumfile | tr -d [:space:] | cut -d"$delim" -f1 | tr [:upper:] [:lower:])
    local rest=$(cat $sumfile | tr -d [:space:] | cut -d"$delim" --complement -f1 | tr [:upper:] [:lower:])
    if [ "$sha512" == "$initial" ] || [ "$sha512" == "$rest" ]; then
        return 0
    fi
    return 1
}

if [ -f $SPARK_HOME/bin/spark-submit ]; then
    echo "Spark is installed, nothing to do"
    exit 1
else
    echo "Attempting to install Spark"
    # If a url has been specfified for spark use it
    if [ -n "$SPARK_URL" ]; then
        echo Downloading $SPARK_URL
        wget $SPARK_URL -P $S2I_SOURCE_DIR
    fi
    if [ -n "$SPARK_SHA512_URL" ]; then
        echo Downloading $SPARK_SHA512_URL
        wget $SPARK_SHA512_URL -P $S2I_SOURCE_DIR
    fi

    for spark in $(ls "$S2I_SOURCE_DIR"); do

        spark=$S2I_SOURCE_DIR/$spark
        echo Found $spark
        echo Checking for valid Spark archive

        # Is the file a directory? If it contains spark-submit, move it
        if [ -d "$spark" ]; then
            if ! [ -f $spark/bin/spark-submit ]; then
                echo Ignoring directory $spark, no spark-submit
                continue
            fi
            echo Installing from directory $spark
            sparkdir=$SPARK_INSTALL/$(basename $spark)
            mv $spark $SPARK_INSTALL
        else
            # If we can get the table of contents, it's a tar archive, otherwise ignore
            tar -tf $spark &> /dev/null
            if [ "$?" -ne 0 ]; then
                echo Ignoring $spark, not a tar archive
                continue
            fi
            echo Validating tar archive $spark

            # Does the tarball contain a spark-submit?
            name=$(tar -tzf $spark | grep -e "^[^/]*/bin/spark-submit$")
            if [ "$?" -ne 0 ]; then
                echo Ignoring tarball $spark, no spark-submit
                continue
            else
                echo Found valid tar archive, matching checksums
                # See if we have an sha512 file to match against
                if [ -f "$spark".sha512 ]; then
                    calcvalue=$(sha512sum "$spark" | cut -d\  -f1)
                    # split the sha512 file using a colon
                    match_sum "$spark".sha512 \:  $calcvalue
                    matched="$?"
                    if [ "$matched" -ne 0 ]; then
                        # split the sha512 file using equals sign in case it's BSD
                        match_sum "$spark".sha512 \=  $calcvalue
                        matched="$?"
                    fi
                    if [ "$matched" -ne 0 ]; then
                        echo Ignoring tarball $spark, sha512sum did not match
                        continue
                    fi
                fi

                # dname will be the intial directory from the path of spark-submit
                # we found in the tarball, ie the dir created by tar
                echo Installing from tarball $spark
                dname=$(dirname $name | cut -d/ -f 1)
                sparkdir=$SPARK_INSTALL/$dname
                tar -xzf $spark -C $SPARK_INSTALL
            fi
        fi

        ln -s $sparkdir $SPARK_INSTALL/distro

        # Search for the spark entrypoint file and copy it to $SPARK_INSTALL
        entry=$(find $sparkdir/kubernetes -name entrypoint.sh)
        if [ -n "$entry" ]; then
            echo Installing spark native entrypoint for use with spark-on-k8s commands
            cp $entry $SPARK_INSTALL

            # We want to get rid of the tini invocation
            sed -i "s@exec .*/tini -s --@exec@" $SPARK_INSTALL/entrypoint.sh
        else
            echo No spark native entrypoint found for use with spark-on-k8s commands
        fi

	# Include the default spark configuration files
        mv --no-clobber "$SPARK_INSTALL"/conf/* "$SPARK_HOME"/conf/

	# If someone included mods in a parallel directory, install them with rsync
        # Don't try to preserve permisions, owner, or group because we don't have
        # any control over how s2i uploaded the files, so there's no use preserving.
        if [ -x /usr/bin/rsync ] && [ -d "$S2I_SOURCE_DIR/modify-spark" ]; then
	    echo Found a modify-spark directory, running rsync to install changes
	    rsync -vrltD "$S2I_SOURCE_DIR/modify-spark/" $SPARK_HOME
        fi

        # Spark workers need to write to the spark directory to track apps
        chmod -R g+rwX $sparkdir

        # Can we run spark-submit?
        $SPARK_HOME/bin/spark-submit --version
        if [ "$?" -eq 0 ]; then
            echo Spark installed successfully
            exit 0
	else
            echo Cannot run spark-submit, Spark install failed
        fi

        # Just in case there is more than one tarball, clean up
        rm -rf $sparkdir
    done

    echo no valid Spark distribution found

    if [ -n "$DEBUG_ASSEMBLE" ]; then
        echo Looping forever so you can \'oc rsh\'
        while true; do
            sleep 5
        done
    fi
    exit 1
fi
