# Copyright 2017 Red Hat
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# ------------------------------------------------------------------------
#
# This is a Dockerfile for the radanalyticsio/openshift-spark:2.3-latest image.

FROM centos:latest

# Environment variables
ENV JBOSS_IMAGE_NAME="radanalyticsio/openshift-spark" \
    JBOSS_IMAGE_VERSION="2.3-latest" \
    PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/spark/bin" \
    SPARK_HOME="/opt/spark" 

# Labels
LABEL name="$JBOSS_IMAGE_NAME" \
      version="$JBOSS_IMAGE_VERSION" \
      architecture="x86_64"  \
      com.redhat.component="radanalyticsio-openshift-spark-docker"  \
      maintainer="Chad Roberts <croberts@redhat.com>" \
      sparkversion="2.3.0" \
      org.concrt.version="1.4.1"


USER root

# Install rh-python36 on centos7
RUN yum install -y centos-release-scl-rh && \
    yum-config-manager --enable centos-sclo-rh && \
    yum install -y rh-python36

# Enable the rh-python36
RUN mkdir -p ${APP_ROOT}/etc
RUN echo "source scl_source enable rh-python36" > ${APP_ROOT}/etc/scl_enable
RUN chmod 666 ${APP_ROOT}/etc/scl_enable
ENV APP_ROOT /opt/app-root
ENV BASH_ENV=${APP_ROOT}/etc/scl_enable \
    ENV=${APP_ROOT}/etc/scl_enable \
    PROMPT_COMMAND=". ${APP_ROOT}/etc/scl_enable"

USER root

# Install required RPMs and ensure that the packages were installed
RUN yum install -y  java-1.8.0-openjdk numpy wget \
    && yum clean all && \
    rpm -q  java-1.8.0-openjdk numpy wget

# Add all artifacts to the /tmp/artifacts
# directory
COPY \
    spark-2.3.0-bin-hadoop2.7.tgz \
    /tmp/artifacts/

# Add scripts used to configure the image
COPY modules /tmp/scripts

# Custom scripts
USER root
RUN [ "bash", "-x", "/tmp/scripts/spark/install" ]

USER root
RUN [ "bash", "-x", "/tmp/scripts/metrics/install" ]

USER root
RUN rm -rf /tmp/scripts
USER root
RUN rm -rf /tmp/artifacts

USER 185

# Specify the working directory
WORKDIR /tmp

ENTRYPOINT ["/entrypoint"]

CMD ["/opt/spark/bin/launch.sh"]
