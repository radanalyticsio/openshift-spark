#!/bin/bash
if [ -f "$SPARK_HOME"/bin/spark-submit ]; then
cat <<EOF
This openshift-spark image will run an Apache Spark master or worker process.
There are several environment variables which affect its operation:

SPARK_MASTER_ADDRESS   -- If this variable is unset, a Spark standalone master
                          process will run.

                          To run a Spark worker process, set this to the URL of
                          the Spark master, for example spark://mymaster:7077

SPARK_METRICS_ON       -- If this variable is unset no metrics sink will be
                          configured.

                          If it's set to 'prometheus' then the Spark process
                          will be started with prometheus metrics on port 7777.

                          If it's set to anything else then the Spark process
                          will be started with jolokia metrics on port 7777.

UPDATE_SPARK_CONF_DIR  -- If this variable is set to a directory, the files in
                          that directory will be copied to $SPARK_HOME/conf
                          before the Spark process starts. This can be used
                          to selectively overwrite Spark configuration files.

                          Note that if $SPARK_HOME/spark-defaults.conf
                          does not contain any explicit settings for parameters
                          matching the pattern 'spark.ui.reverseProxy*', default
                          settings to turn reverse proxy on will be added to
                          the configuration.
EOF

else
cat <<EOF
This is an incomplete openshift-spark image. It contains scripts for running an
Apache Spark master or worker but is missing an actual Spark distribution.

To produce a final image, a source-to-image build must be performed which takes
a Spark distribution as input. This can be done in OpenShift or locally using
the s2i tool if it's installed.

Build inputs:
-------------

The OpenShift method can take either local files or a URL as build input.
For the s2i method, local files are required. Here is an example which
downloads an Apache Spark distribution to a local 'build-input' directory
(including the sha512 file is optional).

$ mkdir build-input
$ wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz -O build-input/spark-2.4.0-bin-hadoop2.7.tgz
$ wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz.sha512 -O build-input/spark-2.4.0-bin-hadoop2.7.tgz.sha512

Optionally, your `build-input` directory may contain a modify-spark directory. The structure of this directory should be parallel to the structure
of the top-level directory in the Spark distribution tarball. The contents of this directory will be copied to the Spark installation using rsync,
allowing you to add or overwrite files. To add my.jar to Spark, for example, put it in build-input/modify-spark/jars/my.jar

Completing the image with OpenShift:
------------------------------------

Run a binary build specifying the spark distribution, for example:

$ oc new-build --name=openshift-spark --docker-image=radanalyticsio/openshift-spark-inc --binary
$ oc start-build openshift-spark --from-file=https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz

This will write the completed image to an imagestream called 'openshift-spark'
in the current project. Note that the value of --from-file can also be a local
directory (see Build Inputs above).

Completing the image with the s2i tool:
---------------------------------------

s2i build build-input radanalyticsio/openshift-spark-inc openshift-spark

This will build a local image named 'openshift-spark:latest' which can
then be uploaded to an image repository.
EOF
fi
